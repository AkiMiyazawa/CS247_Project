CURRENT BEST (total_batch=5000, unsaved): "exp/asymm.uni.cellMax.lstm.concat.dropout.4enc1dec.512x150/stackoverflow.ml.copy.20190611-025738/model/stackoverflow.ml.copy.epoch=1.batch=6000.total_batch=6000.model"

Top-5: 0.5198, Top-10: 0.5136

Param estimates: 1.7GB training ~ 1700mil >> 26mil params (3x1, 200x150)

Commands:
# BASIC 1X1 BIDIRECTIONAL
python train.py -data_path_prefix "data/stackoverflow/stackoverflow" -vocab_path "data/stackoverflow/stackoverflow.vocab.pt" -exp_path "exp/CopyRNN.GRU.concat.input_copy_feeding.1enc1dec/%s.%s" -exp "stackoverflow" -batch_size 64 -bidirectional -copy_attention -copy_mode "concat" -run_valid_every 1000 -save_model_every 10000 -beam_size 16 -beam_search_batch_size 16 -train_ml -attention_mode "concat" -target_attention_mode "concat" -input_feeding -copy_input_feeding -must_appear_in_src -rnn_size 300 -word_vec_size 150 -dropout 0.5 -rnn_type "GRU" -batch_workers 0

# BASIC 2X2 BIDIRECTIONAL
python train.py -data_path_prefix "data/stackoverflow/stackoverflow" -vocab_path "data/stackoverflow/stackoverflow.vocab.pt" -exp_path "exp/lstm.concat.teacher_forcing.input_copy_feeding.2enc2dec/%s.%s" -exp "stackoverflow" -batch_size 64 -bidirectional -copy_attention -copy_mode "concat" -run_valid_every 4000 -save_model_every 10000 -beam_size 16 -beam_search_batch_size 16 -train_ml -attention_mode "concat" -target_attention_mode "concat" -input_feeding -copy_input_feeding -must_appear_in_src -must_teacher_forcing -enc_layers 2 -dec_layers 2 

# ASYMMETRICAL BIDIRECTIONAL
python train.py -data_path_prefix "data/stackoverflow/stackoverflow" -vocab_path "data/stackoverflow/stackoverflow.vocab.pt" -exp_path "exp/asymm.lstm.concat.dropout.2enc1dec.300x150/%s.%s" -exp "stackoverflow" -batch_size 64 -bidirectional -copy_attention -copy_mode "concat" -run_valid_every 1000 -save_model_every 4000 -beam_size 16 -beam_search_batch_size 16 -train_ml -attention_mode "concat" -target_attention_mode "concat" -input_feeding -copy_input_feeding -must_appear_in_src -enc_layers 2 -word_vec_size 150 -dropout 0.5

*************************************** CURRENT BEST ******************************************
# ASYMM UNIDIRECTIONAL 4X1, 512X150, Max Pool -----> 0.5198 (Top-5), 0.5136 (Top-10)
python train.py -data_path_prefix "data/stackoverflow/stackoverflow" -vocab_path "data/stackoverflow/stackoverflow.vocab.pt" -exp_path "exp/asymm.uni.cellMax.lstm.concat.dropout.4enc1dec.512x150/%s.%s" -exp "stackoverflow" -batch_size 64 -copy_attention -copy_mode "concat" -run_valid_every 300 -save_model_every 1000 -beam_size 16 -beam_search_batch_size 16 -train_ml -attention_mode "concat" -target_attention_mode "concat" -input_feeding -copy_input_feeding -must_appear_in_src -enc_layers 4 -word_vec_size 150 -rnn_size 512 -dropout 0.2 -batch_workers 0 -enc_cell_state_method 2 -early_stop_tolerance 20
***********************************************************************************************

**************************************** ML+RL COMMANDS ***************************************
python train.py -data_path_prefix "data/stackoverflow/stackoverflow" -vocab_path "data/stackoverflow/stackoverflow.vocab.pt" -exp_path "exp/asymm.uni.rl.lstm.concat.dropout.3enc1dec.300x150/%s.%s" -exp "stackoverflow" -batch_size 64 -copy_attention -copy_mode "concat" -run_valid_every 2000 -save_model_every 2000 -beam_size 16 -beam_search_batch_size 16 -train_ml -attention_mode "concat" -target_attention_mode "concat" -input_feeding -copy_input_feeding -must_appear_in_src -enc_layers 3 -word_vec_size 150 -dropout 0.5 
-train_rl -rl_start_total_batch 1000 -rl_method 1
-loss_scale 0.2 (0.85 was BAD)
-train_from "exp/asymm.uni.rl.lstm.concat.dropout.3enc1dec.300x150/stackoverflow.ml.rl.copy.20190609-154753/model/stackoverflow.ml.rl.copy.epoch=1.batch=6000.total_batch=6000.model"
***********************************************************************************************

*************************************** UNUSED COMMANDS ***************************************
# -train_rl # -must_teacher_forcing # -word_vec_size 150 # -rnn_size 500 # -train_from 
# -batch_workers 0
***********************************************************************************************

# Plotting
#DONE figure out/fix x axis of plots
     -for epoch in num_epochs
          -for batch in train_data_loader
		train_ml_losses.append(loss_ml)
               -if validating:
	            train_ml_history_losses.append(train_ml_losses)
		    scores_for_plot += [train_ml_history_losses]

#DONE: Figure out how many test/valid samples per validation - 30
#DONE: Set all KeyphraseDataLoader(shuffle=True) in train.py
#DONE: Other attention modes
#DONE: 70:15:15 train, val, test instead

PLAN:
[X] Train and predict unaltered model on kp20k (kp20k.ml.copy.20190520-003217)
[X] Train and predict unaltered model on full kp20k
[X] Try different rnn settings, concat/gru?
[X] Predict unaltered model on stackoverflow dataset.
[X] Train and predict from scratch on stackoverflow dataset
[X] Stacking
[X] try asymmetrical stack
[X] try without must_appear_in_src SUCKS WITHOUT IT	
[X] try without bidirection
[X] try combining cell-states from encoder output
[X] try with more unidirectional encoder layers
[X] try with train_rl (setup a 1-epoch model with RL)	
[X] plot validation loss (Not useful since valid loss is f1@top-5, figure out!!)
[X] try dropout 0.2, add dropout to decoder layer
[X] try max-pooling
[-] enable 1 bidirectional layer
[-] generate table and datapoints to show cellstate processing difference
[-] save model with best test f1score@5

**-copy_input_feeding improves performance slightly**
**2 encoders 2 decoders slightly worse performance**

python preprocess.py -dataset_name "stackoverflow" -source_dataset_dir "data/stackoverflow/json_dump/"

python predict.py -data_path_prefix "data/stackoverflow/stackoverflow" -vocab_path "data/stackoverflow/stackoverflow.vocab.pt" -test_dataset_root_path "data/" -exp "stackoverflow.ml.copy" -train_from "exp/lstm.concat.input_copy_feeding.2enc1dec/stackoverflow.ml.copy.20190603-160113/model/stackoverflow.ml.copy.epoch=1.batch=1500.total_batch=1500.model" -bidirectional -batch_size 16 -beam_search_batch_size 3 -must_appear_in_src 


JSON FORMAT:
{"abstract":"abc def", "keyword":"word 1;word 2;word 3", "title":"title words"}


---------- preprocess.py
line 99-125

---------- config.py
line 25
line 49
line 70
lines 123-132

---------- evaluate.py
line 192-198
line 182
line 234
line 271

---------- train.py
line 189-204 # Don't compute bleu if alpha = 0
line 487 # start rl at total_batch = x instead
line 511

---------- pykp/model.py
line 299 and 305
line 379
line 430
line 440 # Combine encoder hidden/cell states or take last layer????
line 492
line 525
line 789
---------- pykp/io.py

---------- beamsearch.py
line 182-192
line 227
line 243
line 265
line 272